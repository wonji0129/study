{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20210119\n",
    "- review image 데이터를 정답지(Y, N)으로 분류하여 별도 디렉토리에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주피터노트북 화면 넓게 보기 위한 용도\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import boto3\n",
    "import s3fs\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import pymysql\n",
    "from PIL import Image\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() # .env 파일에서 정보를 갖고 오기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "# https://medium.com/@namseok.yoo/django-%EC%97%90%EC%84%9C-python-dotenv-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-6d9b58a939ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(database=os.getenv('database'), user=os.getenv('user'), password=os.getenv('password'), host= 'localhost', port=3306)\n",
    "qry = '''\n",
    "    SELECT a.goods_no, a.est_no, b.image, a.point_yn\n",
    "    FROM \n",
    "    (\n",
    "        SELECT goods_no, no as est_no, point_yn\n",
    "        FROM goods_estimate\n",
    "        WHERE date_format(regi_date, '%Y%m%d') between '20201202' and '20201204'\n",
    "        AND type = 'style' \n",
    "        AND point_yn = 'N'\n",
    "    ) a\n",
    "    JOIN\n",
    "    (\n",
    "        SELECT est_no, image\n",
    "        FROM goods_estimate_image\n",
    "        WHERE date_format(rt, '%Y%m%d') between '20201202' and '20201204'\n",
    "        GROUP BY est_no, image HAVING count(img_no) = 1\n",
    "    ) b\n",
    "    ON a.est_no = b.est_no\n",
    "'''\n",
    "df = pd.read_sql_query(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['point_yn'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20210120\n",
    "### S3에서 데이터 fetch하기 \n",
    "- tf.keras.preprocessing.image_dataset_from_directory를 고려했으나 s3에서 바로 동작하지 않아 -> 아래 아티클 참고하여 진행\n",
    "- https://medium.com/analytics-vidhya/custom-keras-generator-fetching-images-from-s3-to-train-neural-network-4e98694de8ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DB에 저장된 image path를 사용하면 되므로, 다음 과정은 불필요\n",
    "# import pandas as pd\n",
    "# import boto3\n",
    "# from time import time\n",
    "\n",
    "# bucket_name = 'bucket'\n",
    "# client = boto3.client('s3')\n",
    "\n",
    "# test = client.get_paginator('list_objects_v2') # prefix에 특정 key 가져오는거라, 이게 더 복잡한듯\n",
    "# page_iterator = test.paginate(Bucket=os.getenv('bucket'), Prefix=os.getenv('estimate_prefix')\n",
    "# g = pd.Series()\n",
    "# a = time()\n",
    "# n = 0\n",
    "# for page in page_iterator:\n",
    "#     m = [d['Key'] for d in page['Contents'] if d['Key'][-4:] == '.jpg'] \n",
    "#     print(m)\n",
    "#     if len(m) > 0:\n",
    "#         g = g.append(pd.Series(m))\n",
    "#     if len(g) == 10000:\n",
    "#         break\n",
    "\n",
    "# g.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import load_img, img_to_array\n",
    "# import io\n",
    "\n",
    "# def fetch_input(path, s3):\n",
    "#     object = s3.Object(bucket_name, path)\n",
    "#     img = load_img(io.BytesIO(object.get()['Body'].read()))\n",
    "#     return(img)\n",
    "\n",
    "\n",
    "# def preprocess_input(img):\n",
    "#     image = img.resize((128,128))\n",
    "#     array = img_to_array(image)\n",
    "#     return(array)\n",
    "\n",
    "# # I’m using the keras loading module, but properly the common PIL package can be used as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def s3_image_generator(files, batch_size = 16):\n",
    "#     s3 = boto3.resource('s3')\n",
    "#     while True:\n",
    "#         batch_paths = np.array(files)\n",
    "#         batch_input = []\n",
    "#         batch_output = [0] * len(files)\n",
    "#         for input_path in batch_paths:\n",
    "#             input = fectch_input(input_path, s3)\n",
    "#             input = preprocess_input(input)\n",
    "#             batch_input += [ input ]\n",
    "#         batch_x = np.array( batch_input )\n",
    "#         batch_y = np.array( batch_output )\n",
    "#         yield( batch_x, batch_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first = 1\n",
    "# a = time()\n",
    "# s3 = boto3.resource('s3')\n",
    "# for path in g[0:1000]:\n",
    "#     x_batch=[]\n",
    "#     object = s3.Object(bucket_name,path)\n",
    "#     img = load_img(io.BytesIO(object.get()['Body'].read()))\n",
    "#     image = img.resize((128,128))\n",
    "#     array = img_to_arrayㅇ(image)\n",
    "#     preds=(model.predict_on_batch(array))\n",
    "#     if first==1:\n",
    "#         predsA=preds.copy()\n",
    "#         first=0\n",
    "#     else:\n",
    "#         predsA=np.append(predsA,preds,axis=0)\n",
    "# time()-a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s3 -> local -> fetch 밖에 답이 없는듯\n",
    "\n",
    "- 데이터 디렉토리 아래 클래스별로 이미지 나눠서 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 코드만 정상동작\n",
    "\n",
    "def get_image_from_s3(img_path, size=128):\n",
    "    file = boto3.resource('s3').Object(os.getenv('bucket'), img_path[1:]).get()\n",
    "    img = Image.open(file['Body']).resize((size, size))\n",
    "    return img\n",
    "\n",
    "def save_image(img, path, is_array=False):\n",
    "    if is_array:\n",
    "        Image.fromarray(img).save(path)\n",
    "    else:\n",
    "        img.save(path)\n",
    "\n",
    "    return True\n",
    "\n",
    "def proc_image(img_path, est_no, point_yn):\n",
    "    try:\n",
    "        img = get_image_from_s3(img_path)\n",
    "        local_img_path = f'./data/{point_yn.upper()}/{est_no}.jpg'\n",
    "        _ = save_image(img, local_img_path)\n",
    "        img_array = np.array(img) # convert image to numpy array\n",
    "        return img_array\n",
    "    except:\n",
    "        print('error: ', img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_array = df.tail(2000).head(1000).apply(lambda x: proc_image(x['image'], x['est_no'], x['point_yn']), axis=1) # 해당 path에 없는 이미지가 있나봄 - 마지막에 에러남 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20210121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(path) # 하위 디렉토리 하나당 하나의 클래스\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 128\n",
    "batch = 32\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2, \n",
    "    subset='training',\n",
    "    seed=123,\n",
    "    image_size=(img_size, img_size),\n",
    "    batch_size=batch\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2, \n",
    "    subset='validation',\n",
    "    seed=123,\n",
    "    image_size=(img_size, img_size),\n",
    "    batch_size=batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "\n",
    "num_classes = len(class_names)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(images.shape)  # train_dataset - 한 배치당 32개 데이터 포함 (batch_size=32)\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE) \n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (128, 128, 3)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_batch = base_model(image_batch)  # base_model의 output: (None, 5, 5, 1280)\n",
    "\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()  # avg pooling 전략\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "\n",
    "prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "x = normalization_layer(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 10\n",
    "loss0, accuracy0 = model.evaluate(val_ds)\n",
    "\n",
    "history = model.fit(train_ds, epochs=initial_epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Y인데 못 맞춘 이미지\n",
    "class_name = 'Y'\n",
    "for i in y_jpgs:\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        pathlib.Path(f'{path}{class_name}/{i}'), target_size=(128, 128)\n",
    "    )\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "    if model.predict(img_array)[0][0] < 0.2:\n",
    "        img_path = f'{path}{class_name}/{i}'\n",
    "        print(img_path)\n",
    "        display(Image.open(f'{path}{class_name}/{i}'))\n",
    "#     print(model.predict(img_array)[0][0])  ## 이래서 sigmoid를 해야하는거구나"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N인데 못 맞춘 이미지\n",
    "class_name = 'N'\n",
    "for i in n_jpgs:\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        pathlib.Path(f'{path}{class_name}/{i}'), target_size=(128, 128)\n",
    "    )\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "    if model.predict(img_array)[0][0] < 0.3:\n",
    "        img_path = f'{path}{class_name}/{i}'\n",
    "        print(img_path)\n",
    "        display(Image.open(f'{path}{class_name}/{i}'))\n",
    "#     print(model.predict(img_array)[0][0])  ## 이래서 sigmoid를 해야하는거구나"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['est_no']==13545123] # https://store.musinsa.com/app/reviews/views/13545123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
