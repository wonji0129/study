{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommending movies: retrieval\n",
    "- https://www.tensorflow.org/recommenders/examples/basic_retrieval\n",
    "    \n",
    "Real-world recommender systems\n",
    "1. retrieval: selecting an initial set of hundreds of candidates from all possible candidates\n",
    "2. ranking: the outputs of the retrieval model -> the best possible handful of recommendations\n",
    "\n",
    "Retrieval의 두 가지 sub-model\n",
    "1. 쿼리 모델: the query representation (normally a fixed-dimensionality embedding vector)\n",
    "2. 후보 모델: the candidate representation (an equally-sized vector)\n",
    "\n",
    "쿼리모델, 후보모델 결과를 곱해서 -> 쿼리-후보간 선호도 점수 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implicit feedback vs. explicit feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 20.3.4 is available.\n",
      "You should consider upgrading via the '/Users/wonji/.pyenv/versions/3.7.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 20.3.4 is available.\n",
      "You should consider upgrading via the '/Users/wonji/.pyenv/versions/3.7.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement scann\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for scann\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 20.3.4 is available.\n",
      "You should consider upgrading via the '/Users/wonji/.pyenv/versions/3.7.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow-recommenders\n",
    "!pip install -q --upgrade tensorflow-datasets\n",
    "!pip install -q scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pprint\n",
    "import tempfile\n",
    "from typing import Dict, Text\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratings data.\n",
    "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bucketized_user_age': 45.0,\n",
      " 'movie_genres': array([7]),\n",
      " 'movie_id': b'357',\n",
      " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      " 'raw_user_age': 46.0,\n",
      " 'timestamp': 879024327,\n",
      " 'user_gender': True,\n",
      " 'user_id': b'138',\n",
      " 'user_occupation_label': 4,\n",
      " 'user_occupation_text': b'doctor',\n",
      " 'user_rating': 4.0,\n",
      " 'user_zip_code': b'53211'}\n"
     ]
    }
   ],
   "source": [
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_genres': array([4]),\n",
      " 'movie_id': b'1681',\n",
      " 'movie_title': b'You So Crazy (1994)'}\n"
     ]
    }
   ],
   "source": [
    "for x in movies.take(1).as_numpy_iterator():\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "# train 8 test 2\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles = movies.batch(1_000)\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_ids -> integer -> embedding\n",
    "user_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=unique_user_ids, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### candidate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=unique_movie_titles, mask_token=None),\n",
    "  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics & loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = tfrs.metrics.FactorizedTopK(candidates=movies.batch(128).map(movie_model))\n",
    "task = tfrs.tasks.Retrieval(metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.Model):\n",
    "    def __init__(self, user_model, movie_model):\n",
    "        super().__init__()\n",
    "        self.movie_model: tf.keras.Model = movie_model\n",
    "        self.user_model: tf.keras.Model = user_model\n",
    "        self.task: tf.keras.layers.Layer = task\n",
    "            \n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "        return self.task(user_embeddings, positive_movie_embeddings)\n",
    "        # task = tfrs.tasks.Retrieval(metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoBaseClassMovielensModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, user_model, movie_model):\n",
    "        super().__init__()\n",
    "        self.movie_model: tf.keras.Model = movie_model\n",
    "        self.user_model: tf.keras.Model = user_model\n",
    "        self.task: tf.keras.layers.Layer = task\n",
    "            \n",
    "    def train_step(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Loss computation.\n",
    "            user_embeddings = self.user_model(features[\"user_id\"])\n",
    "            positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "            loss = self.task(user_embeddings, positive_movie_embeddings)\n",
    "            \n",
    "            regularization_loss = sum(self.losses)\n",
    "            total_loss = loss + regularization_loss\n",
    "        \n",
    "        # train 단계에만 포함\n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        metrics = {metric.name: metric.result() for metric in self.metrics}\n",
    "        metrics[\"loss\"] = loss\n",
    "        metrics[\"regularization_loss\"] = regularization_loss\n",
    "        metrics[\"total_loss\"] = total_loss\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
    "        # Loss computation.\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "        loss = self.task(user_embeddings, positive_movie_embeddings)\n",
    "\n",
    "        # Handle regularization losses as well.\n",
    "        regularization_loss = sum(self.losses)\n",
    "\n",
    "        total_loss = loss + regularization_loss\n",
    "        metrics = {metric.name: metric.result() for metric in self.metrics}\n",
    "        metrics[\"loss\"] = loss\n",
    "        metrics[\"regularization_loss\"] = regularization_loss\n",
    "        metrics[\"total_loss\"] = total_loss\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovielensModel(user_model, movie_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From /Users/wonji/.pyenv/versions/3.7.7/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5045: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wonji/.pyenv/versions/3.7.7/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5045: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 7s 510ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0013 - factorized_top_k/top_5_categorical_accuracy: 0.0098 - factorized_top_k/top_10_categorical_accuracy: 0.0205 - factorized_top_k/top_50_categorical_accuracy: 0.1006 - factorized_top_k/top_100_categorical_accuracy: 0.1775 - loss: 69885.1129 - regularization_loss: 0.0000e+00 - total_loss: 69885.1129\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 5s 505ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0027 - factorized_top_k/top_5_categorical_accuracy: 0.0190 - factorized_top_k/top_10_categorical_accuracy: 0.0380 - factorized_top_k/top_50_categorical_accuracy: 0.1697 - factorized_top_k/top_100_categorical_accuracy: 0.2930 - loss: 67523.3714 - regularization_loss: 0.0000e+00 - total_loss: 67523.3714\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 5s 501ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0037 - factorized_top_k/top_5_categorical_accuracy: 0.0225 - factorized_top_k/top_10_categorical_accuracy: 0.0459 - factorized_top_k/top_50_categorical_accuracy: 0.1882 - factorized_top_k/top_100_categorical_accuracy: 0.3161 - loss: 66302.9609 - regularization_loss: 0.0000e+00 - total_loss: 66302.9609\n",
      "5/5 [==============================] - 2s 263ms/step - factorized_top_k/top_1_categorical_accuracy: 9.5000e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0099 - factorized_top_k/top_10_categorical_accuracy: 0.0222 - factorized_top_k/top_50_categorical_accuracy: 0.1245 - factorized_top_k/top_100_categorical_accuracy: 0.2325 - loss: 31079.0635 - regularization_loss: 0.0000e+00 - total_loss: 31079.0635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factorized_top_k/top_1_categorical_accuracy': 0.0009500000160187483,\n",
       " 'factorized_top_k/top_5_categorical_accuracy': 0.009850000031292439,\n",
       " 'factorized_top_k/top_10_categorical_accuracy': 0.022199999541044235,\n",
       " 'factorized_top_k/top_50_categorical_accuracy': 0.124549999833107,\n",
       " 'factorized_top_k/top_100_categorical_accuracy': 0.2325499951839447,\n",
       " 'loss': 28244.771484375,\n",
       " 'regularization_loss': 0,\n",
       " 'total_loss': 28244.771484375}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cached_train, epochs=3)\n",
    "model.evaluate(cached_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x154b817d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model) # BruteForce layer is going to be too slow to serve a model with many possible candidates\n",
    "index.index(movies.batch(100).map(model.movie_model), movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 42: [b'Bridges of Madison County, The (1995)'\n",
      " b'Father of the Bride Part II (1995)' b'Rudy (1993)']\n"
     ]
    }
   ],
   "source": [
    "_, titles = index(tf.constant([\"42\"]))\n",
    "print(f\"Recommendations for user 42: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/1c/xhkpty0142l7xwkrqtm3t9p80000gn/T/tmpuwvq7v0r/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/1c/xhkpty0142l7xwkrqtm3t9p80000gn/T/tmpuwvq7v0r/model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations: [b'Bridges of Madison County, The (1995)'\n",
      " b'Father of the Bride Part II (1995)' b'Rudy (1993)']\n"
     ]
    }
   ],
   "source": [
    "# Export the query model.\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    path = os.path.join(tmp, \"model\")\n",
    "    index.save(path) # Save the index.\n",
    "\n",
    "    loaded = tf.keras.models.load_model(path) # Load it back; can also be done in TensorFlow Serving.\n",
    "    scores, titles = loaded([\"42\"])   # Pass a user id in, get top predicted movie titles back.\n",
    "    print(f\"Recommendations: {titles[0][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement scann\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for scann\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 20.3.4 is available.\n",
      "You should consider upgrading via the '/Users/wonji/.pyenv/versions/3.7.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The scann library is not present. Please install it using `pip install scann` to use the ScaNN layer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-cc3747adf7fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscann_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorized_top_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScaNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscann_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmovie_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscann_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"42\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/lib/python3.7/site-packages/tensorflow_recommenders/layers/factorized_top_k.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, query_model, k, distance_measure, num_leaves, num_leaves_to_search, dimensions_per_block, num_reordering_candidates, parallelize_batch_searches, name)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_HAVE_SCANN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m       raise ImportError(\n\u001b[0;32m--> 593\u001b[0;31m           \u001b[0;34m\"The scann library is not present. Please install it using \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m           \u001b[0;34m\"`pip install scann` to use the ScaNN layer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m       )\n",
      "\u001b[0;31mImportError\u001b[0m: The scann library is not present. Please install it using `pip install scann` to use the ScaNN layer."
     ]
    }
   ],
   "source": [
    "scann_index = tfrs.layers.factorized_top_k.ScaNN(model.user_model)\n",
    "scann_index.index(movies.batch(100).map(model.movie_model), movies)\n",
    "\n",
    "\n",
    "_, titles = scann_index(tf.constant([\"42\"]))\n",
    "print(f\"Recommendations for user 42: {titles[0, :3]}\")\n",
    "\n",
    "# Export the query model.\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    path = os.path.join(tmp, \"model\")\n",
    "    \n",
    "    # Save the index.\n",
    "    scann_index.save(\n",
    "        path,\n",
    "        options=tf.saved_model.SaveOptions(namespace_whitelist=[\"Scann\"])\n",
    "    )\n",
    "    # Load it back; can also be done in TensorFlow Serving.\n",
    "    loaded = tf.keras.models.load_model(path)\n",
    "\n",
    "    # Pass a user id in, get top predicted movie titles back.\n",
    "    scores, titles = loaded([\"42\"])\n",
    "\n",
    "    print(f\"Recommendations: {titles[0][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
